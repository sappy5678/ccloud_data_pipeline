{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:52:05.003366Z",
     "start_time": "2020-06-19T14:52:04.025093Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "s = \"Apple is looking at buying U.K. startup for $1 billion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:52:05.071898Z",
     "start_time": "2020-06-19T14:52:05.028194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(s)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:52:08.155094Z",
     "start_time": "2020-06-19T14:52:08.124316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:52:13.672395Z",
     "start_time": "2020-06-19T14:52:13.646408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple nsubj looking\n",
      "U.K. startup startup dobj buying\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T14:54:53.585888Z",
     "start_time": "2020-06-19T14:54:53.550082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(s)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T19:37:52.398040Z",
     "start_time": "2020-06-19T19:37:52.382506Z"
    }
   },
   "outputs": [],
   "source": [
    "output_json = {\n",
    "    'id': \"This is id\",\n",
    "    'title': \"This is input\",\n",
    "    'content': [\n",
    "        {\n",
    "            \"sentence\":{\n",
    "                \"origin\": \"Apple is looking at buying U.K. startup for $1 billion\",\n",
    "                \"Tok\": {},\n",
    "                \"Pos\": {},\n",
    "                \"Tag\": {},\n",
    "                \"Ner\": {},\n",
    "                \"Dep\": {}\n",
    "            }\n",
    "        }\n",
    "    ], \n",
    "    \"metadata\":{}\n",
    "}\n",
    "input_json = {\n",
    "    'id': \"This is id\",\n",
    "    'title': \"This is input\",\n",
    "    'content': [\n",
    "        {\n",
    "            \"sentence\":{\n",
    "                \"origin\": \"Apple is looking at buying U.K. startup for $1 billion\",\n",
    "            }\n",
    "        }\n",
    "    ], \n",
    "    \"metadata\":{}\n",
    "}\n",
    "\n",
    "pipline_json = {\n",
    "       \"id\": \"This is id\",\n",
    "        \"pipline\":{\n",
    "            'Tok': 1 ,\n",
    "            'Pos': 1,\n",
    "            'Tag': 1,\n",
    "            'Ner': 1,\n",
    "            'Dep': 1\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T17:38:48.696190Z",
     "start_time": "2020-06-19T17:38:48.685020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"This is id\",\n",
      "  \"pipline\": {\n",
      "    \"Tokenizer\": 1,\n",
      "    \"Pos\": 1,\n",
      "    \"Tag\": 1,\n",
      "    \"Ner\": 1,\n",
      "    \"Dep\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(pipline_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T20:23:59.710444Z",
     "start_time": "2020-06-19T20:23:59.688668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"This is id\",\n",
      "  \"pipline\": {\n",
      "    \"Tok\": 1,\n",
      "    \"Pos\": 1,\n",
      "    \"Tag\": 1,\n",
      "    \"Ner\": 1,\n",
      "    \"Dep\": 1\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./data/input_pipline/pipline_1.json', 'w') as f:\n",
    "    json.dump(pipline_json, f)\n",
    "\n",
    "with open('./data/input_pipline/pipline_1.json', 'r') as f:\n",
    "    tmp = json.load(f)\n",
    "print(json.dumps(tmp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T17:45:18.444736Z",
     "start_time": "2020-06-19T17:45:18.433782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"This is id\",\n",
      "  \"title\": \"This is input\",\n",
      "  \"Content\": [\n",
      "    {\n",
      "      \"sentence\": {\n",
      "        \"origin\": \"Apple is looking at buying U.K. startup for $1 billion\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./data/input_paragraph/paragraph_1.json', 'w') as f:\n",
    "    json.dump(input_json, f)\n",
    "\n",
    "with open('./data/input_paragraph/paragraph_1.json', 'r') as f:\n",
    "    tmp = json.load(f)\n",
    "print(json.dumps(tmp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T20:40:20.302574Z",
     "start_time": "2020-06-19T20:40:20.289628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"This is id\",\n",
      "  \"title\": \"This is input\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"sentence\": {\n",
      "        \"origin\": \"Apple is looking at buying U.K. startup for $1 billion\",\n",
      "        \"Tok\": {},\n",
      "        \"Pos\": {},\n",
      "        \"Tag\": {},\n",
      "        \"Ner\": {},\n",
      "        \"Dep\": {}\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./data/sample_output.json', 'w') as f:\n",
    "    json.dump(output_json, f)\n",
    "\n",
    "with open('./data/sample_output.json', 'r') as f:\n",
    "    tmp = json.load(f)\n",
    "print(json.dumps(tmp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T20:40:43.916409Z",
     "start_time": "2020-06-19T20:40:43.908083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['content'][0]['sentence']['Tok']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"This is id\",\"title\":\"This is input\",\"content\":[{\"sentence\":{\"origin\":\"Apple is looking at buying U.K. startup for $1 billion\",\"Tok\":[\"Apple\",\"is\",\"looking\",\"at\",\"buying\",\"U.K.\",\"startup\",\"for\",\"$\",\"1\",\"billion\"],\"Pos\":{\"Apple\":\"PROPN\",\"is\":\"AUX\",\"looking\":\"VERB\",\"at\":\"ADP\",\"buying\":\"VERB\",\"U.K.\":\"PROPN\",\"startup\":\"NOUN\",\"for\":\"ADP\",\"$\":\"SYM\",\"1\":\"NUM\",\"billion\":\"NUM\"},\"Tag\":{\"Apple\":\"NNP\",\"is\":\"VBZ\",\"looking\":\"VBG\",\"at\":\"IN\",\"buying\":\"VBG\",\"U.K.\":\"NNP\",\"startup\":\"NN\",\"for\":\"IN\",\"$\":\"$\",\"1\":\"CD\",\"billion\":\"CD\"},\"Ner\":{\"Apple\":\"ORG\",\"U.K.\":\"GPE\",\"$1 billion\":\"MONEY\"},\"Dep\":{\"Apple\":\"nsubj\",\"is\":\"aux\",\"looking\":\"ROOT\",\"at\":\"prep\",\"buying\":\"pcomp\",\"U.K.\":\"compound\",\"startup\":\"dobj\",\"for\":\"prep\",\"$\":\"quantmod\",\"1\":\"compound\",\"billion\":\"pobj\"}}}],\"metadata\":{}}\n"
     ]
    }
   ],
   "source": [
    "url = 'http://140.114.79.144:8000/items'\n",
    "input_json = {\n",
    "    'id': 0,\n",
    "    'title': \"This is input\",\n",
    "    'content': [\n",
    "        {\n",
    "            \"sentence\":{\n",
    "                \"origin\": \"Apple is looking at buying U.K. startup for $1 billion\",\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"pipline\":{\n",
    "        'Tok': 1 ,\n",
    "        'Pos': 1,\n",
    "        'Tag': 1,\n",
    "        'Ner': 1,\n",
    "        'Dep': 1\n",
    "        },\n",
    " \n",
    "    \"metadata\":{}\n",
    "}\n",
    "\n",
    "res = requests.post(url, data = json.dumps(input_json))\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('brown')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Firefox', 'is', 'looking', 'at', 'buying', 'U.K.', 'startup', 'for', '$', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "a_sentence = 'Firefox is looking at buying U.K. startup for $1 billion'\n",
    "print(word_tokenize(a_sentence))\n",
    "\n",
    "\n",
    "# print(pos_tag(word_tokenize(a_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Firefox': 'NNP', 'is': 'VBZ', 'looking': 'VBG', 'at': 'IN', 'buying': 'VBG', 'U.K.': 'NNP', 'startup': 'NN', 'for': 'IN', '$': '$', '1': 'CD', 'billion': 'CD'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "pos = pos_tag(word_tokenize(a_sentence))\n",
    "return_dict = {}\n",
    "for p in pos:\n",
    "    return_dict[p[0]] =p[1]\n",
    "print(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Firefox': 'NOUN', 'is': 'VERB', 'looking': 'VERB', 'at': 'ADP', 'buying': 'VERB', 'U.K.': 'NOUN', 'startup': 'NOUN', 'for': 'ADP', '$': '.', '1': 'NUM', 'billion': 'NUM'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "tag = pos_tag(word_tokenize(a_sentence), tagset='universal')\n",
    "return_dict = {}\n",
    "for t in tag:\n",
    "    return_dict[t[0]] =t[1]\n",
    "print(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180643763"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# url ='https://nlp.stanford.edu/software/stanford-parser-4.0.0.zip'\n",
    "url = 'https://nlp.stanford.edu/software/stanford-ner-4.0.0.zip'\n",
    "# url='https://nlp.stanford.edu/software/stanford-tagger-4.0.0.zip'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('stanford-ner-4.0.0.zip', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268873808"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://nlp.stanford.edu/software/stanford-segmenter-4.0.0.zip'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('stanford-segmenter-4.0.0.zip', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/sappy/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk import Tree\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLASSPATH\"] = \"./StanfordNLP/jars\"\n",
    "os.environ[\"STANFORD_MODELS\"] = \"./StanfordNLP/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-75ef8999635d>:1: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.'\n",
      "  tokenizer = StanfordTokenizer()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = StanfordTokenizer()\n",
    "sent = 'Firefox is looking at buying U.K. startup for $1 billion'\n",
    "print (type(tokenizer.tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default properties from tagger ./StanfordNLP/models/english-bidirectional-distsim.tagger\n",
      "Exception in thread \"main\" edu.stanford.nlp.io.RuntimeIOException: Error while loading a tagger model (probably missing model file)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:937)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:825)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:799)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:322)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.<init>(MaxentTagger.java:305)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1564)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1908)\n",
      "Caused by: java.lang.ClassNotFoundException: edu.stanford.nlp.tagger.maxent.ExtractorNonAlphanumeric\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat java.base/java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:727)\n",
      "\tat java.base/java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1943)\n",
      "\tat java.base/java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1829)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2117)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1646)\n",
      "\tat java.base/java.io.ObjectInputStream.readArray(ObjectInputStream.java:2050)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1634)\n",
      "\tat java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2412)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2306)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2144)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1646)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:464)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.readExtractors(MaxentTagger.java:630)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.readModelAndInit(MaxentTagger.java:876)\n",
      "\t... 6 more\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', './StanfordNLP/jars/stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', './StanfordNLP/models/english-bidirectional-distsim.tagger', '-textFile', '/tmp/tmpyoysla1v', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-571eda488962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meng_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english-bidirectional-distsim.tagger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meng_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(eng_tagger.tag(sent.split()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# Run the tagger and get the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         stanpos_output, _stderr = java(\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stanford_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java command failed : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Java command failed : ['/usr/bin/java', '-mx1000m', '-cp', './StanfordNLP/jars/stanford-postagger.jar', 'edu.stanford.nlp.tagger.maxent.MaxentTagger', '-model', './StanfordNLP/models/english-bidirectional-distsim.tagger', '-textFile', '/tmp/tmpyoysla1v', '-tokenize', 'false', '-outputFormatOptions', 'keepEmptySentences', '-encoding', 'utf8']"
     ]
    }
   ],
   "source": [
    "eng_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "eng_tagger.tag(sent)\n",
    "# print(eng_tagger.tag(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-ner.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-ner.jar, see:\n    <https://nlp.stanford.edu/software>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b550e16a5739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0meng_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english.all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meng_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;34m\"StanfordPOSTagger or StanfordNERTagger?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             )\n\u001b[0;32m---> 71\u001b[0;31m         self._stanford_jar = find_jar(\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m ):\n\u001b[0;32m--> 845\u001b[0;31m     return next(\n\u001b[0m\u001b[1;32m    846\u001b[0m         find_jar_iter(\n\u001b[1;32m    847\u001b[0m             \u001b[0mname_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.2/envs/nlp/lib/python3.8/site-packages/nltk/internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    831\u001b[0m             )\n\u001b[1;32m    832\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n%s\\n%s\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-ner.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-ner.jar, see:\n    <https://nlp.stanford.edu/software>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "eng_tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "return_dict = {}\n",
    "for ner in eng_tagger.tag(sent.split()):\n",
    "    if ner[1] != 'O':\n",
    "        return_dict[ner[0]] = ner[1]\n",
    "print(return_dict)\n",
    "\n",
    "# eng_tagger = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz')\n",
    "# return_dict = {}\n",
    "# for ner in eng_tagger.tag(sentence.split()):\n",
    "#     return_dict[ner[0]] = ner[1]\n",
    "# #             print(return_dict)\n",
    "# return return_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-db3cd0564164>:2: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  eng_parser = StanfordDependencyParser()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "{'looking': 'ROOT', 'Firefox': 'nsubj', 'is': 'aux', 'buying': 'advcl', 'at': 'mark', 'startup': 'obj', 'U.K.': 'compound', 'billion': 'obl', 'for': 'case', '$1': 'compound'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "eng_parser = StanfordDependencyParser()\n",
    "res = list(eng_parser.parse(sent.split()))\n",
    "return_dict = {}\n",
    "turn = True\n",
    "for row in res[0].triples():\n",
    "    print(type(row[1]))\n",
    "    if row[0][0] not in return_dict and turn:\n",
    "        return_dict[row[0][0]] = 'ROOT'\n",
    "        turn = False\n",
    "    return_dict[row[2][0]] = row[1]\n",
    "#     print(row)\n",
    "print(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U.K.': 'GPE', '$1 billion': 'MONEY'}\n"
     ]
    }
   ],
   "source": [
    "import spacy, en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(sent)\n",
    "return_dict = {}\n",
    "for ent in doc.ents:\n",
    "    return_dict[str(ent.text)] = str(ent.label_)\n",
    "print(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Firefox': 'nsubj', 'is': 'aux', 'looking': 'ROOT', 'at': 'prep', 'buying': 'pcomp', 'U.K.': 'compound', 'startup': 'dobj', 'for': 'prep', '$': 'quantmod', '1': 'compound', 'billion': 'pobj'}\n"
     ]
    }
   ],
   "source": [
    "import spacy, en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(sent)\n",
    "return_dict = {}\n",
    "for token in doc:\n",
    "    return_dict[str(token.text)] = str(token.dep_)\n",
    "print(return_dict)\n",
    "\n",
    "#                 os.environ[\"CLASSPATH\"] = \"./StanfordNLP/jars\"\n",
    "#                 os.environ[\"STANFORD_MODELS\"] = \"./StanfordNLP/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CLASSPATH=value\n",
    "!export CLASSPATH=/home/sappy/luigi/ccloud/StanfordNLP/jars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1\n"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> d113409810a9634d308c8867bcb3c9861c4d389d
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.2"
=======
   "version": "3.7.4"
>>>>>>> d113409810a9634d308c8867bcb3c9861c4d389d
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
